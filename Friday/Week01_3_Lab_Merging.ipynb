{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSDS24 - Week 1, Lab 3: Merging and reshaping data\n",
    "\n",
    "In this lab, we will be including two files from the Movie Stack Exchange. The original `'movie_stack_df.feather\"`, but now also `'movie_stack_df_users.feather'`. Depending on your choice of approach to questions 1-3 you may not need the Users database. Both should be available on Canvas.\n",
    "\n",
    "To remind, you can see the schema for this data at the following URL: \n",
    "https://meta.stackexchange.com/questions/2677/database-schema-documentation-for-the-public-data-dump-and-sede\n",
    "\n",
    "This lab will also be the first formative exercise to be submitted for feedback. The details about the deadline will be in the 'assignment' on Canvas. Provisionally, this is due on Wednesday at 5pm. This allows for multiple sessions with the TAs in order to explore and understand your answer to this question. \n",
    "\n",
    "In this lab you will want to merge the posts data in with users data in such a way that you will be able to make some statements about the users that you would not be able to otherwise. Depending on your skill set with managing data, you may want to scale your ambitions accordingly. However, you should be able with some assistance from the web, chat agents, and your peers to draft your own code in order to make some meaningful claims.  \n",
    "\n",
    "For this exercise you will want to display skills in merging, filtering, and aggregation as well as give due regard to operationalisation. We will introduce time series data next week so we should try to consider approaches that do not rely heavily on time series operations. That said, you should be able to filter the data by datetime with little difficulty should you want to explore relationships over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. Considering units of analysis: Posts vs Users \n",
    "\n",
    "In the posts table, each post is a unit of analysis; i.e. a row in our data. Yet, the posts were made by user accounts. We can ask questions of both, either independently or together, where appropriate. In this first exercise we want you to describe a research question that focuses on specific units of analysis.\n",
    "\n",
    "**First** construct a research question (that can be considered with this data) where our unit of analysis is the post. \n",
    "\n",
    "Some examples: \n",
    "- Comparative: Which is longer: a post or an answer?  \n",
    "- Correlational: Do posts with more words lead to answers with more words? \n",
    "\n",
    "**Second** construct a research question where the unit of analysis is the user. \n",
    "\n",
    "Some examples:\n",
    "- Comparative: Do users with a website URL answer posts more often?\n",
    "- Correlational: Do users who write many posts also answer many posts?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1. \n",
    "\n",
    "RQ1. ...\n",
    "\n",
    "RQ2. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator's comments \n",
    " > These will be terse and focus on whether the unit of analysis is correct and whether the other elements make sense as something to be explored in this data.\n",
    "\n",
    "^^^ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2. Operationalisation\n",
    "\n",
    "For only one of these two research questions describe how you will operationalise your concepts.\n",
    "\n",
    "Consider: \n",
    "- Inclusion / exclusion criteria;\n",
    "- How you will establish your unit of analysis derived from the data. For example, for 'longer' posts does this include URLs? Is it by words or by characters? Would that make a difference worth articulating?\n",
    "- What sort of approach might help you establish a statistical difference?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2. \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator's comments \n",
    " > These will be terse and focus on whether the details used to clarify the operationalisation are clear enough that one can see how this could be accomplished with the SE data.\n",
    "\n",
    " ^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3. Performing an operationalisation \n",
    "\n",
    "You are welcome to simply answer the research question if you are curious, but here we have a more modest goal: perform an operationalisation on the data. That is, take one of the proposed measures and get the data in a form where you can provide descriptive statistics about that concept as a variable. For example, for word length you would first count the words per post, which would be either its own column in the DataFrame or its own Series and then plot the distribution of word length. Document any meaningful steps you had to take. In the word length case, we might document how we split the posts into words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Answer 3. \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator's comments \n",
    " > These will focus on the clarity of the description, the extent to which the description matches both the data and the concept, and the clarity of any visualisation of the operationalised feature. Consider discussing the code in terms of FREE.\n",
    "\n",
    "^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4. \"Ackshually\": Partitioning and Aggregating data \n",
    "\n",
    "This is a guided analysis of this data which may or may not be directly useful for your specific research question, but will include several interesting steps involving merging and aggregation. This particular analysis will make use of the fact that comments are threaded, meaning that they do not simply have a post associated with them, but potentially also another comment to which they are replying. \n",
    "\n",
    "There is now an internet meme about people who like to chime in and correct others, often presumably starting their comment with \"Actually, ...\". The meme is typified on knowyourmeme with the standard tropes of neck beard, etc... See: https://knowyourmeme.com/memes/ackchyually-actually-guy\n",
    "\n",
    "In computational social science we can think about structured communication as a series of roles. (See https://www.cmu.edu/joss/content/articles/volume8/Welser/ for an example of how others have operationalised roles). \n",
    "\n",
    "Here we will operationalise roles into 4 different signatures. One of these signatures we may associate with the \"Ackshually\" meme. To identify these signatures we will need to merge the data...with itself. \n",
    "\n",
    "Below I describe the four roles we will identify. These should be mutually exclusive. \n",
    "\n",
    "1. People who never created any content but have a column in the user_df\n",
    "2. People who _only_ create a post but never an answer. \n",
    "3. People who only create an answer. \n",
    "4. People who create both posts and answers. \n",
    "\n",
    "Now here's where it gets tricky: \n",
    "I want you to separate out role number 3 into:\n",
    "3.1. People who only create an answer that is a reply to another answer.\n",
    "3.2. Everyone else who only creates an answer. \n",
    "\n",
    "Then let's find out if people in 3.1 are more prone to using the word \"actually\". As in, these are the people who never ask questions or even provide a useful answer at first, but swoop in to correct someone else's answer. \n",
    "\n",
    "Below I explain why this requires you to merge the data in with itself:\n",
    "- Each post has an \"`Id`\" column indicating its unique index in the data. \n",
    "- Each post has a \"`PostTypeId`\" to delineate whether it is a question or an answer. \n",
    "- Each post has a \"`ParentId`\" column indicating (if it is a reply) what post it is in reply to. If there is a `'Id'` of 3 for one post, and a `'ParentId`' of 3 for another post, that means that the second post is a reply to the first one. \n",
    "\n",
    "You will need to get the PostType for the _parent_ of every answer and merge it in with that answer. Then, if the PostTypeId of the Parent is also 'answer', then this post is a \"reply to an answer\". It is a one to many merge because you are merging the one PostTypeID of the parent into the many answers. \n",
    "\n",
    "Then you have to mark in a separate column the mentions of \"actually\".  \n",
    "\n",
    "Finally, you have split the data into the roles above. There are many possible approaches to this task. Mine would be to mark each post as \"question\", \"answer to a question\", \"answer to another answer\". Then I would sum these per user. Then I would find the roles in this aggregated user data set:\n",
    "\n",
    "Pseudocode: \n",
    "- if (count of \"question\") and (count \"a\" == 0) and (count \"aa\" == 0): group_1\n",
    "- elif (count of \"question\" > 0) and (count \"a\" == 0) and (count \"aa\" == 0): group_2\n",
    "- elif (count of \"question\" > 0) and ((count \"a\" > 0) or (count \"aa\" > 0)): group_4\n",
    "- elif (count of \"question\" == 0) and (count \"a\" == 0) and (count \"aa\" > 0): group_3_1\n",
    "- elif (count of \"question\" == 0) and ((count \"a\" > 0) or (count \"aa\" > 0)): group_3_2\n",
    "\n",
    "Then once I have these roles, I can merge them back into the posts data, and aggregate the mentions of \"actually\" by role type. Then we simply report the findings. You encouraged to try a Chi-Square test of independence on the table with groups 2,3.1,3.2, and 4. If the test is not significant it should mean that no group is more likely than another to use this word. The test is somewhat sensitive to cells with less than 5 in there so this may or may not be suitable, but consider some way to assess whether this we can say there are more observed than expected instances of mentioning the word. \n",
    "\n",
    "> Please note that this lab is on real data with no prior completion of the lab when setting it. Thus, there might not be any interesting relationship between the groups I describe and the word \"actually\" or indeed maybe not even any people in one of the groups I will ask you to create. This is not a trick...this is a deductively created exploration of this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0. Load your data into DataFrames\n",
    "\n",
    "posts_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Create an \"actually\" column. \n",
    "# This can be True or False depending on whether the text contains the word \"actually\". \n",
    "# Structure this in such a way that you can just as easily ask for a different word\n",
    "# This means you should probably create a function and use \"check for word\" as some parameter, then \n",
    "# send check_for_word(\"actually\") so later you can check for another word or phrase.\n",
    "\n",
    "posts_df[\"actually\"] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Identify which rows refer to questions and which refer to answers \n",
    "\n",
    "## For this you can use the post schema: https://meta.stackexchange.com/questions/2677/database-schema-documentation-for-the-public-data-dump-and-sede/326361\n",
    "## To be more direct you will want to filter the data in some manner based on `PostTypeId`\n",
    "## You might want separate `is_post` and `is_answer` columns or perhaps just filter `PostTypeId` removing other rows\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Identify which answer rows reply to an answer or a question\n",
    "# \n",
    "# For this, look to the `ParentId` column. \n",
    "# Now here it can get a bit tricky. You will want to merge in the PostTypeId of the parent's row into the child's row.\n",
    "# Then you can filter the dataset into top-level answers and other other answers. \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Create role-specific labels \n",
    "# See explaination above\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Aggregate the data into the different roles and report on it.\n",
    "# See explaination above \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator's comments\n",
    "\n",
    "> These will focus on the clarity of the code and the plausibility of the result given the code. \n",
    "\n",
    "^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5. Testing a research hypothesis \n",
    "\n",
    "In this part, please reuse your code pipeline in question 4. Except instead of using the word \"actually\", use a word, phrase, or feature of the body text which you think might credibly differ between these four different classes of users (not five since the one's who do not post would not count). For example, in the Movie SE, one might inquire about the use of the word \"cinematic\", for example. Posit a hypothesis that suggests there is a significant difference in the use of `[feature x]` between these four classes. Then report the table and the Chi-square results. Please, try to think of an interesting word/feature and then use that, rather than iterate until you find one that is sigificant. However I understand there is some rationale to exploring different possible words. But try to treat this as a deductive task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5: \n",
    "\n",
    "My hypothesis: ... \n",
    "\n",
    "My rationale: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer 5 Here\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My explanation of the results: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator's comments: \n",
    "\n",
    "> These will focus on the plausibility of the relationship given the code, the clarity of the hypothesis, and the effectiveness of the approach to 'just exploring'. \n",
    "\n",
    "^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
